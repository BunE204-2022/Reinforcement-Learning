---
title: "绪论+马尔科夫决策过程"
author: "吴羽暄"
date: "2022/11/19"
output: word_document
---

## 1 绪论

#### 1.1 这是一本什么书

2016年和2017年最具影响力的AlphaGo大胜世界围棋冠军李世石和柯洁事件，其核心算法就用到了强化学习算法。

第一，本书的语言风格偏口语化；

第二，每章节会接受对应的数学知识，便于理解；

第三，每部分都包括理论讲解，代码讲解和直观解释三项内容（我：先讲理论，再讲代码）；

第四，本书涵盖的内容相当丰富，几乎会涉及强化学习算法的各个方面。

#### 1.2 强化学习可以解决什么问题

序贯决策问题：需要连续不断地做出决策，才能实现最终目标的问题。

```{r}
library(png)
library(grid)
pic1 <- readPNG('D:/pic1.png')
grid.newpage()
vp <- viewport(x = 1.6, y = 1.1, width = 0.2, height = 0.2)
grid.raster(pic1, vp = vp)
```

A:该系统由一个台车（黑体矩形）和两个摆（红色摆杆）组成，可控制的输入为台车的左右运动，该系统的目的是让两级摆稳定在竖直位置。两级摆问题是非线性系统的经典问题，在控制系统理论中，解决该问题的基本思路是先对两级摆系统建立精确的动力学模型，然后基于模型和各种非线性的理论设计控制方法。它需要在每个状态下都有个智能决策（在这里智能决策是指应该施加给台车什么方向、多大的力），以便使整个系统逐渐收敛到目标点（也就是两个摆竖直的状态）。

B:训练好的AlphaGo与柯洁对战的第二局棋。AlphaGo需要根据当前的棋局状
态做出该下哪个子的决策，以便赢得比赛。

C:机器⼈在仿真环境下自己学会了从摔倒的状态爬起来。机器⼈需要得到当前
状态下每个关节的⼒力，以便能够站立起来。

其他领域：视频游戏、人机对话、无人驾驶、机器翻译、文本序列预测等。

综上：强化学习可以解决序贯决策问题。

#### 1.3 强化学习如何解决问题

1.区别于“监督学习”

智能感知问题：以数字手写体识别为例。智能感知其实就是在学习“输入”长得像什么（特征），以及与该长相一一对应的是什么（标签）。所以，智能感知必不可少的前提是需要大量长相差异化的输入以及与输入相关的标签。因此，监督学习解决问题的方法就是输入大量带有标签的数据，让智能体从中学到输入的抽象特征并分类。

序贯决策问题：不关心输入长什么样，只关心当前输入下应该采用什么动作才能实现最终的目标。再次强调，当前采用什么动作与最终的目标有关。也就是说当前采用什么动作，可以使得整个任务序列达到最优。要使整个任务序列达到最优，需要智能体不断地与环境交互，不断尝试。智能体通过动作与环境进行交互时，环境会返给智能体一个当前的回报，智能体则根据当前的回报评估所采取的动作：有利于实现目标的动作被保留，不利于实现目标的动作被衰减。

```{r}
library(png)
library(grid)
pic1 <- readPNG('D:/pic1.3.png')
grid.newpage()
vp <- viewport(x = 1.6, y = 1.3, width = 0.2, height = 0.2)
grid.raster(pic1, vp = vp)
```

用一句话来概括强化学习和监督学习的异同点：强化学习和监督学习的共同点是两者都需要大量的数据进⾏训练，但是两者所需要的数据类型不同。监督学习需要的是多样化的标签数据，强化学习需要的是带有回报的交互数据。由于输⼊的数据类型不同，这就使得强化学习算法有它自己的获取数据、利用数据的独特方法，这些方法在后面的章节中会一一介绍。

2.强化学习算法的发展历史

(1) 第一个关键点是1998年，标志性的事件是Richard S.Sutton出版了他的
强化学习导论第一版，即Reinforcement Learning：An Introduction。该书系统地总结了1998年以前强化学习算法的各种进展。在这一时期强化学习的基本理论框架已经形成。1998年之前，学者们关注和发展得最多的算法是表格型强化学习算法。这一时期基于直接策略搜索的方法也被提出来了。如1992年R.J.Williams提出了Rinforce算法直接对策略梯度进行估计。

(2) 第二个关键点是2013年DeepMind提出DQN（Deep Q Network），将深度网络与强化学习算法结合形成深度强化学习。从1998年到2013年，学者们也没闲着，发展出了各种直接策略搜索的方法。2013年之后，随着深度学习的火热，深度强化学习也越来越引起大家的注意。尤其是2016年和2017年，谷歌的AlphaGo连续两年击败世界围棋冠军，更是将深度强化学习推到了风口浪尖之上。

#### 1.4 强化学习算法分类及发展趋势

1.算法分类

(1) 根据算法是否依赖模型，分为基于模型的强化学习算法和无模型的强化学习算法。共同点：通过与环境交互获得数据；不同点：利用数据的方式不同。基于模型的强化学习算法利用与环境交互得到的数据学习系统或者环境模型，再基于模型进行序贯决策；无模型的强化学习算法则是直接利用与环境交互获得的数据改善自身的行为。两类方法各有优缺点，一般来讲基于模型的强化学习算法效率要比无模型的强化学习算法效率更⾼，因为智能体在探索环境时可以利用模型信息。但是，有些根本无法建立模型的任务只能利用无模型的强化学习算法。由于无模型的强化学习算法不需要建模，所以和基于模型的强化学习算法相比，更具有通用性。

(2) 根据策略的更新和学习方法，强化学习算法可分为基于值函数的强化学习算法、基于直接策略搜索的强化学习算法以及AC的方法。基于值函数的强化学习方法是指学习值函数，最终的策略根据值函数贪婪得到，即任意状态下，值函数最大的动作为当前最优策略。基于直接策略搜索的强化学习算法，一般是将策略参数化，学习实现目标的最优参数。基于AC的⽅法则是联合使用值函数和直接策略搜索。具体的算法会在后面介绍。

(3) 根据环境返回的回报函数是否已知，可以分为正向强化学习和逆向强化学习。在强化学习中，回报函数是人为指定的，回报函数指定的强化学习算法称为正向强化学习。很多时候，回报无法人为指定，如无人机的特效表演，这时可以通过机器学习的方法由函数自己学出来回报。

(4) 本书之外：分层强化学习、元强化学习、多智能体强化学习、关系强化学习和迁移强化学习等。

2.发展趋势

第一，强化学习算法与深度学习的结合会更加紧密。机器学习算法常被分为监督学习、非监督学习和强化学习，如今三类方法联合起来使用效果会更好，谁结合得好，谁就会有更好的突破。这一方向的代表作如基于深度强化学习的对话生成等。

第二，强化学习算法与专业知识结合得将更加紧密。

第三，强化学习算法理论分析会更强，算法会更稳定和高效。

第四，强化学习算法与脑科学、认知神经科学、记忆的联系会更紧密。脑科学和认知神经科学一直是机器学习灵感的源泉，这个源泉往往会给机器学习算法带来革面性的成功。

#### 1.5 强化学习仿真环境构建

学习算法的共同点是从数据中学习，因此数据是学习算法最基本的组成元素。监督学习的数据独立于算法本身，而强化学习的数据是智能体与环境的交互数据，在交互中智能体逐渐地改善行为，产生更好的数据，从而学会技能。也就是说强化学习的数据跟算法是交互的，而非的。因此，相比于监督学习只构建一个学习算法，强化学习还需要构建一个用于与智能体进行交互的环境。

仿真环境必备的两个要素：物理引擎和图像引擎。物理引擎用来计算仿真环境中物体是如何运动的，其背后的原理是物理定律，如刚体动⼒学，
流力学和柔性体动力学等。常用的开源物理引擎有ODE(Open Dynamics Engine)、Bullet、Physx和Havok等。图像引擎则用来显示仿真环境中的物体，包括绘图、渲染等。常⽤的图像引擎大都基于OpenGL(Open Graphics Library)。

本书所用仿真环境为OpenAI的gym。安装使用gym的基本流程：

安装Anaconda(注意，要将路径安装到环境变量中)

→用Anaconda创建虚拟环境(本书只给了Linux系统的安装方法，Windows和Mac自行上网搜)

→安装gym(先激活，再安装各种包)

→下载python/pycharm，将环境设置为gym，import gym即可

具体见代码例1:CartPole实例

#### 1.6 本书主要内容及安排

写作线索：第一条线索是强化学习的基本算法，第二条线索是强化学习算法所用到的基础知识。

1.第一条线索：强化学习的基本算法

强化学习算法解决的是序贯决策问题，而一般的序贯决策问题可以利用马尔科夫决策过程的框架来表述，因此在第2章中介绍了MDP。MDP能够用数学的形式将要解决的问题描述清楚，这也是为什么在介绍强化学习时首先要讲MDP的原因。

第3章介绍基于动态规划的强化学习算法，即对于模型已知的MDP问题的解，并由此引出⼴义策略迭代的⽅法。⼴义策
略迭代⽅法不仅适⽤于基于模型的⽅法，也适⽤于⽆模型的⽅法，是基于值函数强化学习算法的基本框架。因此，第3章是第4章基于蒙特卡罗⽅法、第5章基于时间差分⽅法和第6章基于值函数逼近⽅法的基础。

第4章介绍基于蒙特卡罗的强化学习算法。⽆模型的强化学习算法是整个强化学习算法的核⼼，⽽基于值函数的强化学习算法的核⼼是计算值函数的期望。值函数是个随机变量，其期望的计算可通过蒙特卡罗的⽅法得到。

第5章介绍时间差分⽅法。基于蒙特卡罗的强化学习算法通过蒙特卡罗模拟计算期望，该⽅法需要等到每次试验结束后再对值函数进⾏估计，收敛速度慢。时间差分的⽅法则只需要⼀步便更新，效率⾼、收敛速度快。

第6章介绍基于值函数逼近的强化学习算法。第4章到第5章介绍的是表格型强化学习。所谓表格型强化学习是指状态空间和动作空间都是有限集，动作值函数可⽤⼀个表格来描述，表格的索引分别为状态量和动作量。但是，当状态空间和动作空间很⼤，甚⾄两个空间都是连续空间时，动作值函数已经⽆法使⽤⼀个表格来描述，这时可以⽤函数逼近理论对值函数进⾏逼近。

第7章开始介绍强化学习算法的第二大类：直接策略搜索方法。第7章介绍策略梯度理论、第8章介绍TRPO方法、第9章介绍确定性策略搜索。

第7章到第9章，介绍的是⽆模型的直接策略搜索⽅法。对于机器⼈等复杂系统，⽆模型的⽅法随机初始化很难找到成功的解，因此算法难以收敛。这时，可以利⽤传统控制器来引导策略进⾏搜索。因此第10章介绍了基于引导策略搜索的强化学习算法、为了学习回报函数，第11章介绍了逆向强化学习的算法。

从第12章开始，我们介绍了最近发展出来的强化学习算法，分别是第12章的组合策略梯度和值函数⽅法，第13章的值迭代⽹络和第14章的PILCO⽅法及其扩展。

2.第二条线索：强化学习算法所用到的基础知识

概率学基础、线性方程组的数值求解方法：高斯-赛德尔迭代法、时变与泛函分析中的压缩映射、统计学中的重要技术，如重要性采样、拒绝性采样和MCMC⽅法、基本的函数逼近⽅法：基于⾮参数的函数逼近和基于参数的函数逼近：如卷积神经网络、基本的信息
论概念和基本的优化⽅法、⼤型监督算法常⽤的LBFGS优化算法，及其学习中的并⾏优化算法ADMM算法和KL散度及变分推理等。

## 2 马尔科夫决策过程MDP

#### 2.1 MDP理论讲解

强化学习的学习过程是动态的、不断交互的过程，所需要的数据也是通过与环境不断交互所产⽣的。所以，与监督学习和⾮监督学习相⽐，强化学习涉及的对象更多，⽐如动作，环境，状态转移概率和回报函数等。

组成元素：(S，A，P，R，γ)

S 为有限的状态集

A 为有限的动作集

P 为状态转移概率

R 为回报函数

γ 为折扣因⼦，⽤来计算累积回报

1.马尔科夫性：$P[s_{t+1}|s_t]=P[s_{t+1}|s_1,\dots,s_t]$

(1) 描述每个状态(S)的性质:系统的下⼀个状态$s_{t+1}$仅与当前状态$s_t$有关，⽽与以前的状态⽆关。实际上，当前状态$s_t$其实蕴含了所有相关的历史信息$s_1,\dots,s_t$,⼀旦当前状态已知，历史信息将会被抛弃。

(2) 描述一个状态序列，如：$s_1→s_2→s_3→s_4→s_5$。数学中描述随机变量序列即随机过程。若随机变量序列中的每个状态都是⻢尔科夫的，则称此随机过程为⻢尔科夫随机过程。

2.马尔科夫过程：(S,P)

S是有限状态集合，P是状态转移概率。状态转移概率矩阵为：$$P=
\left [
\begin{matrix}
P_{11} & \cdots & P_{1n}  \\
\vdots & \vdots & \vdots   \\
P_{n1} & \cdots & P_{nn}  
\end{matrix} \right ]
,$$

```{r}
library(png)
library(grid)
pic1 <- readPNG('D:/pic1.3.png')
grid.newpage()
vp <- viewport(x = 1.6, y = 2.2, width = 0.2, height = 0.2)
grid.raster(pic1, vp = vp)
```

如图2.2所⽰为⼀个学⽣的7种状态{娱乐，课程1，课程2，课程3，考过，睡觉，论⽂}，每种状态之间的转换概率如图所⽰。则该生从课程1开始⼀天可能的状态序列为：

课1-课2-课3-考过-睡觉

课1-课2-睡觉

以上状态序列称为⻢尔科夫链。当给定状态转移概率时，从某个状态出发存在多条⻢尔科夫链。对于游戏或者机器⼈，⻢尔科夫过程不⾜以描述其特点，因为不管是游戏还是机器⼈，他们都是通过动作与环境进⾏交互，并从环境中获得奖励，⽽⻢尔科夫过程中不存在动作和奖励。将动作（策略）和回报考虑在内的⻢尔科夫过程称为⻢尔科夫决策过程。

3.马尔科夫决策过程：$P_{ss'}^a=P[S_{t+1}=s'|S_t=s,A_t=a]$

(1) 描述MDP：{S,A,P,R,$\gamma$}

```{r}
library(png)
library(grid)
pic1 <- readPNG('D:/pic1.3.png')
grid.newpage()
vp <- viewport(x = 1.6, y = 2.3, width = 0.2, height = 0.2)
grid.raster(pic1, vp = vp)
```

马尔科夫过程只描述状态S，而MDP的状态转移概率还包含动作A。如图2.3所示，学⽣的：

状态集为S={$s_1,s_2,s_3,s_4,s_5$}

动作集为A={玩，退出，学习，发表，睡觉}

用R表示图2.3中(每个动作对应的)⽴即回报。

累积回报$G_t=R_{t+1}+\gamma R_{t+2}+\cdots=\sum_{k=0}^ \infty \gamma^k R_{t+k+1}$(注：给定策略π时可以计算)

(2) 随机策略

强化学习的⽬标是给定MDP，寻找最优即总回报最大的策略。

策略：一个条件概率。即$\pi(a|s)=p[A_t=a|S_t=s]$，状态到动作的映射，即策略π在每个状态s指定⼀个动作a概率或给定状态s时，动作集上的⼀个分布。常⽤符号π表⽰。

例如其中⼀个学⽣的策略为$\pi_1(玩|s_1)$=0.8，即该学⽣在状态$s_1$时玩的概率为0.8，不玩的概率是0.2，显然这个学⽣更喜欢玩。另外⼀个学⽣的策略为$\pi_2(玩|s_1)$=0.3，即该学⽣在状态$s_1$时玩的概率是0.3，显然这个学⽣不爱玩。依此类推，每个学⽣都有⾃⼰的策略。

当给定策略π时，假设从状态$s_1$出发，学⽣状态序列可能为

$s_1→s_2→s_3→s_4→s_5$

$s_1→s_2→s_3→s_5$

$\vdots$

此时，在策略π下，利⽤累计回报公式可以计算累积回报$G_1$ ，此时$G_1$有多个可能值。由于策略π是随机的，因此累积回报也是随机的。

为了评价状态$s_1$的价值，我们需要定义⼀个确定量来描述状态$s_1$的价值，很⾃然的想法是利⽤累积回报来衡量状态$s_1$的价值。然⽽，累积回报$G_1$是个随机变量，不是⼀个确定值，因此⽆法描述，但其期望是个确定值，可以作为状态值函数的定义。

(3) 状态值函数与状态-行为值函数的定义式

状态值函数：累积回报在状态s处的期望值
$$\begin{aligned}
v_\pi(s)&=E_\pi[G_t|S_t=s]\\
&=E_\pi[\sum_{k=0}^\infty\gamma^kR_{t+k+1}|S_t=s]\\
\end{aligned}$$
因为策略π决定了累计回报G的状态分布，故状态值函数是与策略π相对应的。

状态-行为值函数：累积回报在状态s及动作a处的期望值
$$\begin{aligned}
q_\pi(s,a)&=E_\pi[G_t|S_t=s,A_t=a]\\
&=E_\pi[\sum_{k=0}^\infty\gamma^kR_{t+k+1}|S_t=s,A_t=a]\\
\end{aligned}$$

(4) 状态值函数与状态-行为值函数的贝尔曼方程

$$\begin{aligned}
v_\pi(S_t)&=E[G_t|S_t=s]\\
&=E[R_{t+1}+\gamma v(S_{t+1})|S_t=s]\\
&=\sum_{a\in A}\sum_{s'\in S} \pi(a|s)(R_s^a+\gamma P_{ss'}^av_\pi(s'))\\
\end{aligned}$$

$$\begin{aligned}
q(S_t,A_t)&=E[G_t|S_t=s,A_t=a]\\
&=E[R_{t+1}+\gamma q(S_{t+1},A_{t+1})|S_t=s,A_t=a]\\
&=R_s^a+\gamma \sum_{s'\in S}\sum_{a'\in A}P_{ss'}^a \pi(a'|s')q_\pi(s',a')\\
\end{aligned}$$
表明了当前状态s值函数与下个状态s'值函数的关系。实际上给出了计算值函数的方法：迭代/递归

因为$G_t=R_{t+1}+\gamma G_{t+1}$，故t时刻计算的值函数必然和t+1时刻的值函数存在关系

```{r}
library(png)
library(grid)
pic1 <- readPNG('D:/pic1.3.png')
grid.newpage()
vp <- viewport(x = 1.6, y = 2.4, width = 0.2, height = 0.2)
grid.raster(pic1, vp = vp)
```

(5)最优状态值函数、状态-行为值函数及最优策略
$$\begin{aligned}
v^*(s)&=\max_\pi v_\pi(s)\\
&=\max_a R_s^a+\gamma \sum_{s' \in S}P_{ss'}^av^*(s')\\
\end{aligned}$$

$$\begin{aligned}
q^*(s,a)&=\max_\pi q_\pi(s,a)\\
&=R_s^a +\gamma \sum_{s' \in S}P_{ss'} \max_{a'}q^*(s',a')\\
\end{aligned}$$
最优策略选择：$$\pi^*(a|s)=\begin{cases}1\quad if \quad a=\arg \max \limits_ {a\in A} q^*(s,a) \\0\quad othewise\end{cases}$$


#### 2.2 MDP中的概率学基础讲解

1. 理解随机策略$\pi(a|s)=p[A_t=a|S_t=s]$

(1)随机变量：MDP中随机变量指的是当前的动作，⽤字⺟a表⽰，可以是离散也可以是连续的。

(2)概率分布：⽤来描述随机变量在每个可能取到的值处的可能性大小。指定⼀个策略π就是指定取每个动作a的概率。

(3)条件概率：策略$\pi(a|s)$指在当前状态π处，采取某个动作a的概率。当给定随机变量后，状态s处的累积回报G(s)也是随机变量，⽽且其分布由随机策略π决定。状态值函数定义为该累积回报的期望。

(4)期望

离散型：$$E_{x\sim P}[f(x)]=\sum_xP(x)f(x)$$
连续型：$$E_{x\sim P}[f(x)]=\int p(x)f(x)dx$$

(5)方差：采样值差异的大小$$Var(f(x))=E[(f(x)-E[f(x)])^2]$$

2. 常用的随机策略(概率分布)

(1) 贪婪策略
$$\pi^*(a|s)=\begin{cases}1 \quad if \quad a=\arg \max \limits_ {a\in A} q^*(s,a) \\0 \quad othewise\end{cases}$$一个确定性策略，即只有在使得状态-动作值函数最大的动作a处取概率为1。

(2) $\epsilon-greedy$策略
$$\pi(a|s)\leftarrow\begin{cases}1-\epsilon+\frac{\epsilon}{|A(s)|} \quad if \quad a=\arg \max \limits_ {a\in A} Q(s,a) \\0 \quad\quad\quad\quad\quad\quad if \quad a\neq \arg \max \limits_ {a\in A} Q(s,a)\end{cases}$$选取使得状态-动
作值函数最⼤的动作的概率为$1-\epsilon+\frac{\epsilon}{|A(s)|}$，而其他动作概率为等概率$\frac{\epsilon}{|A(s)|}$

(3) 高斯策略
$$\pi_\theta=\mu_\theta+\epsilon,\epsilon\sim N(0,\sigma^2)$$其中，$\mu_0$为确定性部分，$\epsilon$为零均值的高斯随机噪声，应用于连续型变量。

(4) 玻尔兹曼分布
$$\pi(a|s,\theta)= \frac{exp(Q(s,a,\theta))}{\sum_bexp(h(s,b,\theta))} $$其中Q(s,a,$\theta$)为动作值函数。应用于动作空间是离散的或者动作空间并不⼤的情况。该策略的含义是，动作值函数⼤的动作被选中的概率⼤，动作值函数⼩的动作被选中的概率⼩。

#### 2.3 基于gym的MDP实例讲解

见代码例2:找金币
